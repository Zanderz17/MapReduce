{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1b201556-e8b9-49ad-9d12-34b183451512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "CPU times: user 59 µs, sys: 19 µs, total: 78 µs\n",
      "Wall time: 80.6 µs\n"
     ]
    }
   ],
   "source": [
    "def find_longest_string(list_of_strings):\n",
    "    longest_string = None\n",
    "    longest_string_len = 0 \n",
    "    for s in list_of_strings:\n",
    "        if len(s) > longest_string_len:\n",
    "            longest_string_len = len(s)\n",
    "            longest_string = s\n",
    "    return longest_string\n",
    "\n",
    "list_of_strings = ['abc', 'python', 'dima']\n",
    "%time max_length = print(find_longest_string(list_of_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d566f095-aa62-4156-b049-819121dc5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar los datos\n",
    "news = fetch_20newsgroups()\n",
    "\n",
    "# Crear un DataFrame con los datos\n",
    "df = pd.DataFrame({'text': news.data, 'target': news.target})\n",
    "\n",
    "data = news.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c550bf79-93de-4b8b-a16c-01830e769425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "def clean_word(word):\n",
    "    return re.sub(r'[^\\w\\s]','',word).lower()\n",
    "    \n",
    "def word_not_in_stopwords(word):\n",
    "    return word not in ENGLISH_STOP_WORDS and word and word.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "dac79f4b-90a0-4542-a0d0-816d58fa2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_top_words(data):\n",
    "    cnt = Counter()\n",
    "    for text in data:\n",
    "        tokens_in_text = text.split()\n",
    "        tokens_in_text = map(clean_word, tokens_in_text)\n",
    "        tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "        cnt.update(tokens_in_text)\n",
    "        \n",
    "    return cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "963a41ee-e556-4b4a-b0c2-dd7dedebcb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.15 s, sys: 0 ns, total: 3.15 s\n",
      "Wall time: 3.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('subject', 12252),\n",
       " ('lines', 11824),\n",
       " ('organization', 11185),\n",
       " ('writes', 7836),\n",
       " ('article', 6754),\n",
       " ('people', 5832),\n",
       " ('dont', 5813),\n",
       " ('like', 5757),\n",
       " ('just', 5579),\n",
       " ('university', 5544)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time find_top_words(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c57d5-0983-47ae-b395-95a8c2b594b9",
   "metadata": {},
   "source": [
    "# Using MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbbc53-5bea-4f32-9edf-d373ecf94a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(text):\n",
    "    tokens_in_text = text.split()\n",
    "    tokens_in_text = map(clean_word, tokens_in_text)\n",
    "    tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "    return Counter(tokens_in_text)\n",
    "\n",
    "def reducer(cnt1, cnt2):\n",
    "    cnt1.update(cnt2)\n",
    "    return cnt1\n",
    "\n",
    "def chunk_mapper(chunk):\n",
    "    mapped = map(mapper, chunk)\n",
    "    reduced = reduce(reducer, mapped)\n",
    "    return reduced\n",
    "\n",
    "def chunkify(text, number_of_chunks):\n",
    "    return [text[i:i+number_of_chunks] for i in range(0, len(text), number_of_chunks)]\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import reduce\n",
    "pool = Pool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7497897e-55fa-4625-b235-1955a490fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%%time\n",
    "data_chunks = chunkify(data, number_of_chunks=36)\n",
    "#step 1:\n",
    "mapped = pool.map(chunk_mapper, data_chunks)\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf84ffa-d642-4b77-9337-ce3297de4ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
